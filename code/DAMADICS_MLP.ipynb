{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Test notebook for the damadics benchmark. Approach using ANN (MLP). \n",
    "\n",
    "First we import the necessary packages and create the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:damadics.dataManagement:Connection to mysql+mysqldb://localhost:3306/damadics2 successfull\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import logging\n",
    "import random\n",
    "import plottingTools\n",
    "import TF_MLP\n",
    "import utils\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from dataManagement import DataManagerDamadics\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib notebook\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(\"damadics\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "handler = logging.FileHandler(\"DataManager.log\")\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "random_seed = 0 #Change this to make it really random, 0 for testing purposes\n",
    "random.seed(random_seed)\n",
    "\n",
    "columnArrangement = ['id', 'selectedFault', 'faultType', 'faultIntensity', 'externalControllerOutput', 'disturbedMediumFlow', \n",
    "                        'pressureValveInlet', 'pressureValveOutlet', 'mediumTemperature', 'rodDisplacement']\n",
    "\n",
    "desiredComponents = [\"Valve\"]\n",
    "dataManager = DataManagerDamadics(user=\"readOnly\", password=\"readOnly\", engineType=\"mysql+mysqldb://\", \n",
    "                                  dbName=\"damadics2\", host=\"localhost\", port=\"3306\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data\n",
    "\n",
    "We retrieve the data needed for our models. We specify the start and end dates for the data we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:damadics.dataManagement:Loading data for components [<class 'damadicsDBMapping.ValveReading'>] from 2017-12-30 00:00:00 to 2018-01-28 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sample size 41121, Train Size 37008 (90.0%), Test Size 4113 (10.0%)\n",
      "Total sample size 41121, Faulty samples 6256, Normal samples 34865, Fault/Non Fault Ratio 0.1794\n"
     ]
    }
   ],
   "source": [
    "startDateTime = datetime(2017, 12, 30, hour=0, minute=0, second=0, microsecond=0)\n",
    "endDateTime = datetime(2018, 1, 28, hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "train_size = 0.9\n",
    "test_size = 1-train_size\n",
    "\n",
    "#0 means normal and 1 means fault\n",
    "X, y, df = dataManager.retrieve_and_reshape_data(startDateTime, endDateTime, desiredComponents, columnArrangement)\n",
    "\n",
    "#Standardize the data\n",
    "X_transformed = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, train_size = train_size, \n",
    "                                                    test_size = test_size, random_state=random_seed)\n",
    "\n",
    "#Generate useful counts\n",
    "n_m, n_x = X.shape #Number of samples, features\n",
    "train_n_m, train_n_x = X_train.shape #Number of samples, features for training\n",
    "test_n_m, test_n_x = X_test.shape #Number of samples, features for testing\n",
    "fault_count = np.count_nonzero(y)\n",
    "non_fault_count = n_m - fault_count\n",
    "fault_nofault_Ratio = fault_count/non_fault_count\n",
    "\n",
    "print('\\nTotal sample size {}, Train Size {} ({}%), Test Size {} ({}%)'.format(n_m, X_train.shape[0], np.round(train_size*100),  \n",
    "                                                                               X_test.shape[0], np.round(test_size*100)))\n",
    "\n",
    "print('Total sample size {}, Faulty samples {}, Normal samples {}, Fault/Non Fault Ratio {:.4f}'.\n",
    "      format(n_m, fault_count, non_fault_count, fault_nofault_Ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn implementation\n",
    "\n",
    "Now that I have the data, i'll try various implementations. Lets start with SCIKIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 20\n",
      "0.196303201482\n",
      "Mean train accuracy 0.9296098140942499\t Mean test accuracy 0.9316800389010454\n",
      "Precision (train) 0.9274992919852733\t Precision (test) 0.9312039312039312\n",
      "Recall (train) 0.582325746799431\t Recall (test) 0.5996835443037974\n",
      "F1 (train) 0.7154560349535773\t F1 (test) 0.7295476419634264\n"
     ]
    }
   ],
   "source": [
    "#using adam optimizer for applying minibatches\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(10, 10, 5), activation='relu', solver='adam', alpha=1e-2, batch_size=512, \n",
    "                       learning_rate='constant', learning_rate_init=1e-2)\n",
    "\n",
    "#Fit the MLP to the data.\n",
    "y_train_sk = y_train.ravel()\n",
    "y_test_sk = y_test.ravel()\n",
    "\n",
    "mlp_clf.fit(X_train, y_train_sk)\n",
    "\n",
    "print('Number of iterations: {}'.format(mlp_clf.n_iter_))\n",
    "print(mlp_clf.loss_)\n",
    "\n",
    "y_pred_train = mlp_clf.predict(X_train)\n",
    "y_pred_test = mlp_clf.predict(X_test)\n",
    "\n",
    "#Compute metrics\n",
    "score_train = mlp_clf.score(X_train, y_train_sk)\n",
    "score_test = mlp_clf.score(X_test, y_test_sk)\n",
    "precision_train = precision_score(y_train_sk, y_pred_train)\n",
    "precision_test = precision_score(y_test_sk, y_pred_test)\n",
    "recall_train = recall_score(y_train_sk, y_pred_train)\n",
    "recall_test = recall_score(y_test_sk, y_pred_test)\n",
    "f1_train = f1_score(y_train_sk, y_pred_train)\n",
    "f1_test = f1_score(y_test_sk, y_pred_test)\n",
    "\n",
    "print(\"Mean train accuracy {}\\t Mean test accuracy {}\".format(score_train, score_test))\n",
    "print(\"Precision (train) {}\\t Precision (test) {}\".format(precision_train, precision_test))\n",
    "print(\"Recall (train) {}\\t Recall (test) {}\".format(recall_train, recall_test))\n",
    "print(\"F1 (train) {}\\t F1 (test) {}\".format(f1_train, f1_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TF implementation\n",
    "\n",
    "Lets now try with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{}\n",
      "{'W1': <tf.Variable 'W1:0' shape=(10, 6) dtype=float32_ref>, 'W2': <tf.Variable 'W2:0' shape=(10, 10) dtype=float32_ref>, 'W3': <tf.Variable 'W3:0' shape=(5, 10) dtype=float32_ref>, 'W4': <tf.Variable 'W4:0' shape=(1, 5) dtype=float32_ref>}\n",
      "{'b1': <tf.Variable 'b1:0' shape=(10, 1) dtype=float32_ref>, 'b2': <tf.Variable 'b2:0' shape=(10, 1) dtype=float32_ref>, 'b3': <tf.Variable 'b3:0' shape=(5, 1) dtype=float32_ref>, 'b4': <tf.Variable 'b4:0' shape=(1, 1) dtype=float32_ref>}\n",
      "Cost after epoch 0: 0.494060\n",
      "Cost after epoch 100: 0.260074\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tfmlp_clf = TF_MLP.MLPClassifier(hidden_layer_sizes=(10,10,5))\n",
    "X_tf = X.T\n",
    "y_tf = y.T\n",
    "#y_tf = tf.cast(y.T, tf.float32)\n",
    "tfmlp_clf.fit(X_tf, y_tf)\n",
    "\n",
    "print(\"Finished training\")\n",
    "\n",
    "#W1 = tf.get_variable(\"W1\", [25, 12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#print(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 41121)\n",
      "(1, 41121)\n",
      "(6, 41121)\n",
      "(1, 41121)\n",
      "[[ 1.        1.        0.222181 ...,  0.794534  0.147993  0.876782]\n",
      " [ 0.87684   0.850576  0.849537 ...,  0.848818  0.847772  0.847832]\n",
      " [ 0.650832  0.643717  0.647472 ...,  0.644823  0.64161   0.643922]\n",
      " [ 0.214553  0.215373  0.214781 ...,  0.213406  0.216033  0.218695]\n",
      " [ 0.00159   0.369939  0.732446 ...,  0.47581   0.81031   0.458222]\n",
      " [ 0.5       0.366043  0.732444 ...,  0.257854  0.659356  0.484302]]\n",
      "[[0 0 0 ..., 0 0 0]]\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 256)\n",
      "(1, 256)\n",
      "(6, 161)\n",
      "(1, 161)\n"
     ]
    }
   ],
   "source": [
    "X_tf = X.T\n",
    "y_tf = y.T\n",
    "\n",
    "print(X_tf.shape)\n",
    "print(y_tf.shape)\n",
    "\n",
    "minibatches = utils.random_mini_batches(X_tf, y_tf, 256, 1)\n",
    "\n",
    "for minibatch in minibatches:\n",
    "    (mini_X, mini_y) = minibatch\n",
    "    print(mini_X.shape)\n",
    "    print(mini_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

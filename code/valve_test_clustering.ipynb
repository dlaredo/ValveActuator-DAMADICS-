{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime\n",
    "import CMAPSAuxFunctions\n",
    "\n",
    "from data_handler_VALVE import ValveDataHandler\n",
    "from tunable_model import SequenceTunableModelRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Reshape, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create Data Handler </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['timestamp', 'externalControllerOutput', 'undisturbedMediumFlow', 'pressureValveInlet', 'pressureValveOutlet', 'mediumTemperature', 'rodDisplacement', 'disturbedMediumFlow', 'selectedFault', 'faultType', 'faultIntensity']\n",
    "selected_indices = np.array([2,4,5,6,7,8])\n",
    "selected_features = list(features[i] for i in selected_indices-1)\n",
    "\n",
    "window_size = 30\n",
    "window_stride = 1\n",
    "\n",
    "# min = 2018-02-14 18:59:20\n",
    "# max = 2018-08-19 18:28:20\n",
    "time_start = \"2018-02-14 18:59:20\"\n",
    "time_end = \"2018-04-19 18:28:20\"\n",
    "\n",
    "# Either classification or regression\n",
    "problem = 'classification'\n",
    "#problem = 'regression'\n",
    "\n",
    "vHandler = ValveDataHandler(time_start, time_end, selected_features = selected_features,\n",
    "                            sequence_length = window_size, sequence_stride = window_stride,\n",
    "                            problem = problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Keras Model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "K.clear_session()\n",
    "lambda_regularization = 0.20\n",
    "\n",
    "def create_ANN_model(input_shape, problem):\n",
    "    \n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(20, input_dim = input_shape, activation = 'relu', kernel_initializer = 'glorot_normal', \n",
    "                    kernel_regularizer = regularizers.l2(lambda_regularization), name = 'fc1'))\n",
    "    \n",
    "    if (problem == 'classification'):\n",
    "        model.add(Dense(1, activation = 'softmax', name = 'out'))\n",
    "    elif (problem == 'regression'):\n",
    "        model.add(Dense(1, activation = 'linear', name = 'out'))\n",
    "        \n",
    "    #model.add(Dense(1, activation = 'softmax', name = 'out'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tunable Model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range = (-1, 1))\n",
    "#scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model(shape, problem):\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    if (problem == 'classification'):\n",
    "        # Parameters for the model\n",
    "        optimizer = SGD(lr = 0.01, momentum = 0.0, decay = 0.0, nesterov = False)\n",
    "        loss_function = 'categorical_crossentropy'\n",
    "        metrics = ['accuracy']\n",
    "    elif (problem == 'regression'):\n",
    "        # Parameters for the model\n",
    "        optimizer = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad = False)\n",
    "        loss_function = 'mean_squared_error'\n",
    "        metrics = ['mse']\n",
    "        \n",
    "    model = None\n",
    "    \n",
    "    # Create and compile the model\n",
    "    model = create_ANN_model(shape, problem)\n",
    "    model.compile(optimizer = optimizer, loss = loss_function, metrics = metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(selected_features)\n",
    "input_shape = num_features * window_size\n",
    "\n",
    "model = get_compiled_model(input_shape, problem)\n",
    "tModel = SequenceTunableModelRegression('ANN_Model', model, lib_type = 'keras', data_handler = vHandler)\n",
    "\n",
    "tModel.data_handler.data_scaler = None\n",
    "tModel.data_scaler = scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Loading Data from MySQL Database </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vHandler.connect_to_db(\"remoteAdmin\",\"remoteAdmin\",\"169.236.181.40:3306\",\"damadics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from database\n",
    "#vHandler.extract_data_from_db()\n",
    "# vHandler.extract_data_from_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from .csv file. If loading from .csv, don't forget to uncomment .extract_data_from_db() in data_handler_VALVE'''\n",
    "vHandler._df = pd.read_csv('valve_dataset.csv', sep = ',')\n",
    "for i in range(1,9):\n",
    "    vHandler._df[vHandler._df.columns[-i]] = vHandler._df[vHandler._df.columns[-i-1]]\n",
    "vHandler._df['undisturbedMediumFlow'] = pd.np.nan\n",
    "\n",
    "if (problem == 'classification'):\n",
    "    # For anomaly detection, the standard is to indicate inliners as '1' and outliers as '-1'\n",
    "    vHandler._df['status'] = vHandler._df['selectedFault'].apply(lambda valve: 1 if valve == 20 else -1)\n",
    "elif (problem == 'regression'):\n",
    "    vHandler._y = vHandler._df[\"selectedFault\"].values\n",
    "\n",
    "# adds a new feature called \"Status\" on whether the valve is broken '1' or not broken '0'\n",
    "#vHandler._df['status'] = vHandler._df['selectedFault'].apply(lambda valve: 0 if valve == 20 else 1)\n",
    "# this adds a column to vHandler._X\n",
    "#vHandler._X = vHandler._df[selected_features]\n",
    "#vHandler._X.insert(loc = 0, column = 'id', value = np.arange(len(vHandler._df)))\n",
    "# vHandler._X = vHandler._X.values\n",
    "\n",
    "# vHandlezr._y = vHandler._df['status'].values\n",
    "# print(vHandler._df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vHandler._df['id'] = np.arange(1, vHandler._df.shape[0] + 1)\n",
    "# print(vHandler._df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = MinMaxScaler(feature_range = (-1, 1))\n",
    "scaler = StandardScaler()\n",
    "\n",
    "vHandler._X = vHandler._df[selected_features].values\n",
    "\n",
    "old_X = tModel.data_handler._X\n",
    "tModel.data_handler._X = scaler.fit_transform(tModel.data_handler._X)\n",
    "\n",
    "print('Data Set')\n",
    "print('Before: {}'.format(old_X[0][0:5]))\n",
    "print('After: {}\\n'.format(tModel.data_handler._X[0][0:5]))\n",
    "\n",
    "if (problem == 'classification'):\n",
    "    vHandler._y = vHandler._df[\"status\"].values\n",
    "elif (problem == 'regression'):\n",
    "    vHandler._y = vHandler._df[\"selectedFault\"].values\n",
    "        \n",
    "# vHandler._X = vHandler._df[selected_features].values\n",
    "# vHandler._y = vHandler._df[\"selectedFault\"].values\n",
    "#vHandler._y = list(map(lambda valve: 0 if valve == 20 else 1, vHandler._y))\n",
    "#vHandler._y = vHandler._df[\"status\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vHandler._X)\n",
    "# print(vHandler._y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vHandler.load_data(cross_validation_ratio = 0.3, test_ratio = 0.2, unroll = True)\n",
    "tModel.load_data(unroll = True, verbose = 0, cross_validation_ratio = 0.2, test_ratio = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add this class onto data_handler_VALVE\n",
    "\n",
    "# if (problem == 'classification'):\n",
    "broken_counter = 0\n",
    "not_broken_counter = 0\n",
    "dataset = vHandler._y_crossVal\n",
    "print(dataset.shape)\n",
    "#print(vHandler._X_test[1])\n",
    "# print(vHandler._y_test)\n",
    "\n",
    "for i in dataset:\n",
    "    #print(i)\n",
    "    if (i == 1.0):\n",
    "        not_broken_counter += 1;\n",
    "    else:\n",
    "        broken_counter += 1;\n",
    "\n",
    "print(\"Broken Counter:\", broken_counter)\n",
    "print(\"Not Broken Counter:\", not_broken_counter)\n",
    "\n",
    "broken_counter = 0\n",
    "not_broken_counter = 0\n",
    "dataset = vHandler._y_test\n",
    "print(dataset.shape)\n",
    "#print(vHandler._X_test[1])\n",
    "# print(vHandler._y_test)\n",
    "\n",
    "for i in dataset:\n",
    "    #print(i)\n",
    "    if (i == 1.0):\n",
    "        not_broken_counter += 1;\n",
    "    else:\n",
    "        broken_counter += 1;\n",
    "\n",
    "print(\"Broken Counter:\", broken_counter)\n",
    "print(\"Not Broken Counter:\", not_broken_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vHandler.print_data(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> One Class SVM </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "svm = OneClassSVM(kernel='rbf', degree=5, gamma='scale',\n",
    "                  coef0=0.0, tol=0.001, nu=0.5,\n",
    "                  shrinking=True, cache_size=200, verbose=True,\n",
    "                  max_iter=-1)\n",
    "svm.fit(tModel.data_handler._X_train)\n",
    "print('Time Elapsed: {}'.format(datetime.now()- start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_crossVal = svm.predict(tModel.data_handler._X_crossVal)\n",
    "y_pred_test = svm.predict(tModel.data_handler._X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_crossVal = metrics.accuracy_score(tModel.data_handler._X_crossVal, y_pred_crossVal)\n",
    "precision_crossVal = metrics.precision_score(tModel.data_handler._X_crossVal, y_pred_crossVal)\n",
    "recall_crossVal = metrics.recall_score(tModel.data_handler._X_crossVal, y_pred_crossVal)\n",
    "confusion_matrix_crossVal = metrics.confusion_matrix(tModel.data_handler._X_crossVal, y_pred_crossVal)\n",
    "\n",
    "print(\"Accuracy for cross-validation:\", acc_crossVal)\n",
    "print(\"Precision for cross-validation:\", precision_crossVal)\n",
    "print(\"Recall for cross-validation:\", recall_crossVal)\n",
    "print(\"Confusion matrix for cross-validation\\n\", confusion_matrix_crossVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test = metrics.accuracy_score(tModel.data_handler._X_test, y_pred_test)\n",
    "precision_test = metrics.precision_score(tModel.data_handler._X_test, y_pred_test)\n",
    "recall_test = metrics.recall_score(tModel.data_handler._X_test, y_pred_test)\n",
    "confusion_matrix_test = metrics.confusion_matrix(tModel.data_handler._X_test, y_pred_test)\n",
    "\n",
    "print(\"Accuracy for test:\", acc_test)\n",
    "print(\"Precision for test:\", precision_test)\n",
    "print(\"Recall for test:\", recall_test)\n",
    "print(\"Confusion matrix for test\\n\", confusion_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Isolation Forest </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# default\n",
    "# IsolationForest(n_estimators=100, max_samples='auto', contamination='legacy',\n",
    "#                 max_features=1.0, bootstrap=False, n_jobs=None,\n",
    "#                 behaviour=’old’, random_state=None, verbose=0)\n",
    "\n",
    "start_time = datetime.now()\n",
    "iso_forest = IsolationForest(n_estimators = 100, max_samples = 'auto', contamination = 'legacy',\n",
    "                             max_features = 1.0, bootstrap = False, n_jobs = None, behaviour = 'new',\n",
    "                             random_state = None, verbose = 1)\n",
    "iso_forest.fit(tModel.data_handler._X_train)\n",
    "print('Time Elapsed: {}'.format(datetime.now()- start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iso_forest.decision_function(tModel.data_handler._X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_crossVal = iso_forest.predict(tModel.data_handler._X_crossVal)\n",
    "y_pred_test = iso_forest.predict(tModel.data_handler._X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_crossVal = metrics.accuracy_score(tModel.data_handler._y_crossVal, y_pred_crossVal)\n",
    "precision_crossVal = metrics.precision_score(tModel.data_handler._y_crossVal, y_pred_crossVal)\n",
    "recall_crossVal = metrics.recall_score(tModel.data_handler._y_crossVal, y_pred_crossVal)\n",
    "confusion_matrix_crossVal = metrics.confusion_matrix(tModel.data_handler._y_crossVal, y_pred_crossVal)\n",
    "\n",
    "print(\"Accuracy for cross-validation:\", acc_crossVal)\n",
    "print(\"Precision for cross-validation:\", precision_crossVal)\n",
    "print(\"Recall for cross-validation:\", recall_crossVal)\n",
    "print(\"Confusion matrix for cross-validation\\n\", confusion_matrix_crossVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test = metrics.accuracy_score(tModel.data_handler._y_test, y_pred_test)\n",
    "precision_test = metrics.precision_score(tModel.data_handler._y_test, y_pred_test)\n",
    "recall_test = metrics.recall_score(tModel.data_handler._y_test, y_pred_test)\n",
    "confusion_matrix_test = metrics.confusion_matrix(tModel.data_handler._y_test, y_pred_test)\n",
    "\n",
    "print(\"Accuracy for test:\", acc_test)\n",
    "print(\"Precision for test:\", precision_test)\n",
    "print(\"Recall for test:\", recall_test)\n",
    "print(\"Confusion matrix for test\\n\", confusion_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del iso_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Robust Covariance/Elliptic Envelope </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "EE = EllipticEnvelope(store_precision=True, assume_centered=False, support_fraction=None,\n",
    "                      contamination=0.1, random_state=None)\n",
    "EE.fit(tModel.data_handler._X_train)\n",
    "print('Time Elapsed: {}'.format(datetime.now()- start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_crossVal = EE.predict(tModel.data_handler._X_crossVal)\n",
    "y_pred_test = EE.predict(tModel.data_handler._X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_crossVal = metrics.accuracy_score(tModel.data_handler._y_crossVal, y_pred_crossVal)\n",
    "precision_crossVal = metrics.precision_score(tModel.data_handler._y_crossVal, y_pred_crossVal)\n",
    "recall_crossVal = metrics.recall_score(tModel.data_handler._y_crossVal, y_pred_crossVal)\n",
    "confusion_matrix_crossVal = metrics.confusion_matrix(tModel.data_handler._y_crossVal, y_pred_crossVal)\n",
    "\n",
    "print(\"Accuracy for cross-validation:\", acc_crossVal)\n",
    "print(\"Precision for cross-validation:\", precision_crossVal)\n",
    "print(\"Recall for cross-validation:\", recall_crossVal)\n",
    "print(\"Confusion matrix for cross-validation\\n\", confusion_matrix_crossVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test = metrics.accuracy_score(tModel.data_handler._y_test, y_pred_test)\n",
    "precision_test = metrics.precision_score(tModel.data_handler._y_test, y_pred_test)\n",
    "recall_test = metrics.recall_score(tModel.data_handler._y_test, y_pred_test)\n",
    "confusion_matrix_test = metrics.confusion_matrix(tModel.data_handler._y_test, y_pred_test)\n",
    "\n",
    "print(\"Accuracy for test:\", acc_test)\n",
    "print(\"Precision for test:\", precision_test)\n",
    "print(\"Recall for test:\", recall_test)\n",
    "print(\"Confusion matrix for test\\n\", confusion_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

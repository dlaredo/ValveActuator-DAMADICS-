{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Initialization\n",
    "\n",
    "Test notebook for the damadics benchmark. Approach using anomaly detection techniques. \n",
    "\n",
    "First we import the necessary packages and create the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import logging\n",
    "import random\n",
    "import plottingTools\n",
    "import sys\n",
    "import datetime\n",
    "import graphviz \n",
    "import datetime\n",
    "\n",
    "sys.path.append('/media/controlslab/DATA/Projects')\n",
    "#sys.path.append('/Users/davidlaredorazo/Documents/University_of_California/Research/Projects')\n",
    "\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns; sns.set(font_scale=1.2)\n",
    "\n",
    "from ann_framework.data_handlers.data_handler_DAMADICS import DamadicsDataHandler\n",
    "\n",
    "#Import the tunable model classes\n",
    "from ann_framework.tunable_model.tunable_model import SequenceTunableModelClassification\n",
    "\n",
    "\n",
    "#from IPython.display import display, HTML\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup some options for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0 #Change this to make it really random, 0 for testing purposes\n",
    "random.seed(random_seed)\n",
    "\n",
    "y_trains = {'DummyClf':None, 'EllipticEnvelope':list(), 'MLP':None}\n",
    "y_tests = {'DummyClf':None, 'EllipticEnvelope':list(), 'MLP':None}\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "nsamples = 1000\n",
    "scoringMetrics = ['precision_macro', 'recall_macro', 'f1_macro']\n",
    "cv_folds = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data hanlder and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-14 17:06:41\n",
      "2019-10-31 14:26:41\n",
      "['externalControllerOutput', 'pressureValveInlet', 'pressureValveOutlet', 'mediumTemperature', 'rodDisplacement']\n"
     ]
    }
   ],
   "source": [
    "start_date_test = datetime.datetime(2018, 2, 14, 18, 59, 20) # ValveReadingsTest, testing\n",
    "start_date_training = datetime.datetime(2019, 6, 14, 17, 6, 41) # ValveReadings, trainning\n",
    "time_delta = datetime.timedelta(days=0, seconds=0, microseconds=0, milliseconds=0, minutes=1, hours=0, weeks=0)\n",
    "\n",
    "n = 200000\n",
    "\n",
    "end_date_training = start_date_training + n*time_delta #get the first n instances\n",
    "end_date_test = start_date_test + n*time_delta #get the first n instances\n",
    "\n",
    "print(start_date_training)\n",
    "print(end_date_training)\n",
    "\n",
    "features = ['externalControllerOutput', 'undisturbedMediumFlow', 'pressureValveInlet', \n",
    "            'pressureValveOutlet', 'mediumTemperature', 'rodDisplacement', 'disturbedMediumFlow', \n",
    "           'selectedFault', 'faultType', 'faultIntensity']\n",
    "\n",
    "selected_indices = np.array([1,3,4,5,6])\n",
    "selected_features = list(features[i] for i in selected_indices-1)\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to mysql+mysqldb://readOnly:_readOnly2019@169.236.181.40/damadics successfull\n"
     ]
    }
   ],
   "source": [
    "#Does not work for sequence sizes larger than 1 given the way I'm generating the test data. \n",
    "#Need to properly define what the test data is going to be like.\n",
    "window_size = 1\n",
    "window_stride = 1\n",
    "\n",
    "dHandlder_valve_tree = DamadicsDataHandler(selected_features, window_size, window_stride,\n",
    "                                      start_date_training=start_date_training, end_date_training=end_date_training,\n",
    "                                      start_date_test=start_date_test, end_date_test=end_date_test,\n",
    "                                      binary_classes=True, one_hot_encode=False, samples_per_run=50)\n",
    "dHandlder_valve_tree.connect_to_db('readOnly', '_readOnly2019', '169.236.181.40', 'damadics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for the first time\n",
      "Reloading data due to parameter change\n",
      "Loading training data for DAMADICS with window_size of 1, stride of 1. Cros-Validation ratio 0.5\n",
      "Loading data from database\n",
      "Reading data from ValveReading\n",
      "2019-06-14 17:06:41\n",
      "2019-10-31 14:26:41\n",
      "Extracting data from database runtime: 0:00:01.364530\n",
      "Data Splitting: 0:00:00.000093\n",
      "Loading test data for DAMADICS with window_size of 1, stride of 1\n",
      "Loading data from database\n",
      "Reading data from ValveReadingTest\n",
      "Extracting data from database runtime: 0:00:01.235509\n",
      "Printing shapes\n",
      "\n",
      "Training data (X, y)\n",
      "(94033, 5)\n",
      "(94033, 1)\n",
      "Cross-Validation data (X, y)\n",
      "(3533, 5)\n",
      "(3533, 1)\n",
      "Testing data (X, y)\n",
      "(7771, 5)\n",
      "(7771, 1)\n",
      "Printing first 5 elements\n",
      "\n",
      "Training data (X, y)\n",
      "[[0.257854 0.848348 0.653717 0.214572 0.497933]\n",
      " [0.257854 0.848305 0.656842 0.213451 0.510227]\n",
      " [0.484302 0.85034  0.640007 0.216292 0.455605]\n",
      " [0.484302 0.849299 0.647105 0.214206 0.458444]\n",
      " [0.257854 0.850806 0.649526 0.216032 0.476276]]\n",
      "[[-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]]\n",
      "Cross-Validation data (X, y)\n",
      "[[0.484302 0.850503 0.653546 0.217067 0.472696]\n",
      " [0.366043 0.849794 0.65316  0.215119 0.672703]\n",
      " [0.484302 0.851608 0.647947 0.211988 0.58216 ]\n",
      " [0.732444 0.849476 0.647334 0.218086 0.729278]\n",
      " [0.257854 0.849436 0.643103 0.214984 0.466857]]\n",
      "[[-1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [-1.]\n",
      " [-1.]]\n",
      "Testing data (X, y)\n",
      "[[0.732444 0.849129 0.655813 0.213983 0.73182 ]\n",
      " [0.732444 0.84726  0.654143 0.213793 0.998883]\n",
      " [0.732444 0.84722  0.162056 0.214349 0.725755]\n",
      " [0.484302 0.850775 0.653471 0.216841 0.999878]\n",
      " [0.732444 0.848964 0.657332 0.214426 0.999099]]\n",
      "[[-1.]\n",
      " [-1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "tModel = SequenceTunableModelClassification('Damadics_Tree_SK', tree_clf, lib_type='scikit', \n",
    "                                            data_handler=dHandlder_valve_tree)\n",
    "\n",
    "#tModel.data_scaler = scaler\n",
    "\n",
    "tModel.load_data(unroll=True, verbose=1, cross_validation_ratio=0.5, shuffle_samples=True)\n",
    "tModel.print_data(print_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "training_set = np.concatenate((tModel.X_train, tModel.y_train), axis=1)\n",
    "cv_set = np.concatenate((tModel.X_crossVal, tModel.y_crossVal), axis=1)\n",
    "test_set = np.concatenate((tModel.X_test, tModel.y_test), axis=1)\n",
    "\n",
    "np.savetxt(\"training_set.csv\", training_set, delimiter=\",\")\n",
    "np.savetxt(\"cv_set.csv\", cv_set, delimiter=\",\")\n",
    "np.savetxt(\"test_set.csv\", test_set, delimiter=\",\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform classification using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tModel.train_model(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3533,)\n",
      "On test set\n",
      "0.6340220775544863\n",
      "On train set\n",
      "0.8356534408133315\n"
     ]
    }
   ],
   "source": [
    "tModel.predict_model(cross_validation=True)\n",
    "tModel.evaluate_model(cross_validation=True)\n",
    "\n",
    "print(tModel.y_predicted.shape)\n",
    "\n",
    "predicted = tModel.y_predicted\n",
    "test = tModel.y_crossVal\n",
    "test = np.ravel(test)\n",
    "\n",
    "print(\"On test set\")\n",
    "print(accuracy_score(test, predicted))\n",
    "\n",
    "clf = tModel.model\n",
    "y_train_pred = clf.predict(tModel.X_train)\n",
    "\n",
    "train = tModel.y_train\n",
    "train = np.ravel(train)\n",
    "\n",
    "print(\"On train set\")\n",
    "print(accuracy_score(tModel.y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['externalControllerOutput', 'pressureValveInlet', 'pressureValveOutlet', 'mediumTemperature', 'rodDisplacement']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'decision_tree_damadics.pdf'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = features[:1] + features[2:6]\n",
    "print(feature_names)\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None,\n",
    "                                feature_names=feature_names,  \n",
    "                                class_names=[\"normal\", \"fault\"],  \n",
    "                                filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "#graph \n",
    "\n",
    "graph.render('decision_tree_damadics', view=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with time window\n",
    "\n",
    "Use the mean of the time window to do the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to mysql+mysqldb://readOnly:_readOnly2019@169.236.181.40/damadics successfull\n",
      "Loading data for the first time\n",
      "Reloading data due to parameter change\n",
      "Loading training data for DAMADICS with window_size of 10, stride of 1. Cros-Validation ratio 0.5\n",
      "Loading data from database\n",
      "Reading data from ValveReading\n",
      "2019-06-14 17:06:41\n",
      "2019-10-31 14:26:41\n",
      "Extracting data from database runtime: 0:00:01.361327\n",
      "Data Splitting: 0:00:00.000096\n",
      "Loading test data for DAMADICS with window_size of 10, stride of 1\n",
      "Loading data from database\n",
      "Reading data from ValveReadingTest\n",
      "Extracting data from database runtime: 0:00:01.236513\n",
      "Printing shapes\n",
      "\n",
      "Training data (X, y)\n",
      "(108682, 10, 5)\n",
      "(108682, 1)\n",
      "Cross-Validation data (X, y)\n",
      "(3239, 10, 5)\n",
      "(3239, 1)\n",
      "Testing data (X, y)\n",
      "(7424, 10, 5)\n",
      "(7424, 1)\n",
      "Printing first 5 elements\n",
      "\n",
      "Training data (X, y)\n",
      "[[[0.732444 0.849492 0.64463  0.214594 0.718367]\n",
      "  [0.257854 0.851315 0.642411 0.213998 0.466731]\n",
      "  [0.659356 0.849536 0.646299 0.215309 0.810019]\n",
      "  [0.484302 0.845377 0.64903  0.212424 0.461047]\n",
      "  [0.366043 0.850045 0.652005 0.215299 0.669205]\n",
      "  [0.732444 0.851001 0.653164 0.213399 0.746533]\n",
      "  [0.257854 0.852071 0.656044 0.215302 0.511713]\n",
      "  [0.659356 0.849556 0.657798 0.21717  0.830675]\n",
      "  [0.484302 0.850843 0.65409  0.214636 0.471669]\n",
      "  [0.366043 0.850247 0.646484 0.214774 0.652959]]\n",
      "\n",
      " [[0.732444 0.85146  0.64329  0.214931 0.728904]\n",
      "  [0.257854 0.850893 0.648637 0.215097 0.257813]\n",
      "  [0.659356 0.847988 0.648859 0.218583 0.779826]\n",
      "  [0.484302 0.847846 0.654781 0.214371 0.466224]\n",
      "  [0.732444 0.850266 0.65376  0.214408 0.744081]\n",
      "  [0.257854 0.847887 0.65254  0.214304 0.273983]\n",
      "  [0.659356 0.849534 0.64893  0.216345 0.777625]\n",
      "  [0.484302 0.852224 0.646074 0.217008 0.456147]\n",
      "  [0.366043 0.847249 0.641679 0.216992 0.487393]\n",
      "  [0.732444 0.851197 0.644506 0.215009 0.726857]]\n",
      "\n",
      " [[0.659356 0.847508 0.646122 0.215969 0.806341]\n",
      "  [0.484302 0.847882 0.648695 0.214813 0.463887]\n",
      "  [0.366043 0.84676  0.651948 0.214766 0.656117]\n",
      "  [0.732444 0.848752 0.656813 0.215256 0.753722]\n",
      "  [0.257854 0.849995 0.657425 0.214679 0.4725  ]\n",
      "  [0.659356 0.851268 0.654368 0.214663 0.82933 ]\n",
      "  [0.484302 0.849862 0.650189 0.214633 0.460859]\n",
      "  [0.366043 0.849019 0.648299 0.214744 0.628622]\n",
      "  [0.732444 0.851015 0.647078 0.214504 0.719983]\n",
      "  [0.257854 0.846823 0.641052 0.216305 0.429053]]\n",
      "\n",
      " [[0.484302 0.848775 0.646572 0.21583  0.458525]\n",
      "  [0.366043 0.846934 0.645444 0.212879 0.651332]\n",
      "  [0.732444 0.845019 0.64664  0.215184 0.728034]\n",
      "  [0.257854 0.847168 0.649418 0.214005 0.481388]\n",
      "  [0.659356 0.848321 0.650085 0.215481 0.815304]\n",
      "  [0.484302 0.84973  0.654675 0.214052 0.471234]\n",
      "  [0.366043 0.848615 0.658062 0.216081 0.673356]\n",
      "  [0.732444 0.847282 0.654213 0.214746 0.735723]\n",
      "  [0.257854 0.852031 0.651249 0.212899 0.501805]\n",
      "  [0.659356 0.849407 0.650544 0.214227 0.827084]]\n",
      "\n",
      " [[0.659356 0.8494   0.645307 0.216177 0.820401]\n",
      "  [0.484302 0.849629 0.643448 0.219109 0.454597]\n",
      "  [0.366043 0.850048 0.643798 0.214999 0.65663 ]\n",
      "  [0.732444 0.8488   0.64663  0.21594  0.729011]\n",
      "  [0.257854 0.849086 0.65092  0.214448 0.488001]\n",
      "  [0.659356 0.849604 0.654191 0.214833 0.819453]\n",
      "  [0.484302 0.850319 0.651638 0.215046 0.473726]\n",
      "  [0.366043 0.851131 0.655079 0.214482 0.666906]\n",
      "  [0.732444 0.851016 0.653593 0.214141 0.729299]\n",
      "  [0.257854 0.84783  0.650506 0.215499 0.497213]]]\n",
      "[[ 1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]]\n",
      "Cross-Validation data (X, y)\n",
      "[[[0.366043 0.847142 0.645544 0.21651  0.652613]\n",
      "  [0.732444 0.85025  0.643102 0.213079 0.720467]\n",
      "  [0.257854 0.850053 0.64278  0.216335 0.466861]\n",
      "  [0.659356 0.849418 0.645593 0.212941 0.805761]\n",
      "  [0.484302 0.850688 0.649779 0.214127 0.462439]\n",
      "  [0.366043 0.848798 0.653666 0.214178 0.671414]\n",
      "  [0.732444 0.850529 0.653163 0.219123 0.748116]\n",
      "  [0.659356 0.85024  0.65705  0.216914 0.832456]\n",
      "  [0.484302 0.849589 0.650312 0.215728 0.467628]\n",
      "  [0.366043 0.849704 0.648247 0.215885 0.651633]]\n",
      "\n",
      " [[0.484302 0.849902 0.654769 0.213766 0.596308]\n",
      "  [0.366043 0.848929 0.656019 0.214191 0.72129 ]\n",
      "  [0.732444 0.84855  0.653456 0.214366 0.884974]\n",
      "  [0.257854 0.84905  0.649666 0.216426 0.462358]\n",
      "  [0.659356 0.850058 0.646335 0.215051 0.9377  ]\n",
      "  [0.484302 0.845921 0.646949 0.215125 0.567906]\n",
      "  [0.366043 0.848823 0.64386  0.213678 0.675721]\n",
      "  [0.732444 0.849676 0.646016 0.214338 0.827293]\n",
      "  [0.257854 0.849229 0.647897 0.216305 0.46365 ]\n",
      "  [0.659356 0.849763 0.653812 0.21374  0.937261]]\n",
      "\n",
      " [[0.257854 0.849416 0.64718  0.21382  0.429987]\n",
      "  [0.659356 0.846966 0.647775 0.217344 0.916034]\n",
      "  [0.366043 0.846618 0.655212 0.214917 0.68072 ]\n",
      "  [0.732444 0.849533 0.657712 0.217097 0.885668]\n",
      "  [0.257854 0.848887 0.652128 0.212728 0.448735]\n",
      "  [0.659356 0.849932 0.650213 0.215641 0.925126]\n",
      "  [0.484302 0.848373 0.646064 0.21656  0.577204]\n",
      "  [0.366043 0.846359 0.646462 0.216468 0.648993]\n",
      "  [0.732444 0.849745 0.644189 0.215205 0.837906]\n",
      "  [0.257854 0.847671 0.643902 0.216242 0.430193]]\n",
      "\n",
      " [[0.659356 0.847464 0.643047 0.214331 0.811628]\n",
      "  [0.484302 0.850075 0.642655 0.215615 0.456659]\n",
      "  [0.366043 0.846981 0.648356 0.214929 0.668522]\n",
      "  [0.732444 0.846257 0.652779 0.214901 0.741961]\n",
      "  [0.257854 0.851255 0.65561  0.217023 0.505051]\n",
      "  [0.659356 0.846611 0.656177 0.215181 0.830644]\n",
      "  [0.484302 0.848876 0.654252 0.214653 0.475823]\n",
      "  [0.366043 0.849553 0.652884 0.214361 0.659245]\n",
      "  [0.732444 0.848898 0.647772 0.217866 0.719196]\n",
      "  [0.257854 0.85011  0.645597 0.216195 0.476362]]\n",
      "\n",
      " [[0.484302 0.847542 0.646772 0.212495 0.457676]\n",
      "  [0.366043 0.848937 0.642409 0.214914 0.651843]\n",
      "  [0.732444 0.850324 0.643998 0.21664  0.725325]\n",
      "  [0.257854 0.849706 0.646307 0.212704 0.485601]\n",
      "  [0.659356 0.849724 0.652396 0.215059 0.813766]\n",
      "  [0.484302 0.851417 0.6543   0.216821 0.472739]\n",
      "  [0.366043 0.849336 0.657499 0.215665 0.672782]\n",
      "  [0.732444 0.848245 0.655334 0.214368 0.734041]\n",
      "  [0.257854 0.847992 0.654364 0.212932 0.505885]\n",
      "  [0.659356 0.852524 0.648489 0.214923 0.82617 ]]]\n",
      "[[-1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [-1.]\n",
      " [ 1.]]\n",
      "Testing data (X, y)\n",
      "[[[0.366043 0.848083 0.655453 0.215324 1.      ]\n",
      "  [0.732444 0.847913 0.651183 0.214188 1.      ]\n",
      "  [0.257854 0.845335 0.64739  0.214328 0.999428]\n",
      "  [0.659356 0.849175 0.642463 0.217047 0.999315]\n",
      "  [0.484302 0.849358 0.644724 0.213035 1.      ]\n",
      "  [0.366043 0.851574 0.646538 0.215141 0.997863]\n",
      "  [0.732444 0.850749 0.652581 0.215628 1.      ]\n",
      "  [0.257854 0.848472 0.653124 0.215079 0.99833 ]\n",
      "  [0.659356 0.847881 0.655493 0.215508 0.999306]\n",
      "  [0.484302 0.849496 0.654367 0.213911 0.999433]]\n",
      "\n",
      " [[0.659356 0.849094 0.644553 0.214812 0.99775 ]\n",
      "  [0.484302 0.848321 0.644779 0.218133 0.998632]\n",
      "  [0.366043 0.848272 0.645765 0.216055 1.      ]\n",
      "  [0.732444 0.849394 0.648788 0.215369 1.      ]\n",
      "  [0.257854 0.848557 0.65424  0.216445 1.      ]\n",
      "  [0.659356 0.849359 0.654564 0.218768 0.997541]\n",
      "  [0.484302 0.84845  0.656528 0.214374 0.99899 ]\n",
      "  [0.366043 0.850888 0.653684 0.216592 1.      ]\n",
      "  [0.732444 0.84997  0.64793  0.213774 0.999128]\n",
      "  [0.257854 0.850579 0.644701 0.21566  0.999012]]\n",
      "\n",
      " [[0.484302 0.848705 0.649434 0.214998 1.      ]\n",
      "  [0.366043 0.851274 0.651649 0.214134 0.998069]\n",
      "  [0.732444 0.852127 0.656841 0.215574 0.999365]\n",
      "  [0.257854 0.849077 0.655563 0.213997 1.      ]\n",
      "  [0.659356 0.851527 0.655289 0.213538 0.999069]\n",
      "  [0.484302 0.849039 0.650373 0.214816 1.      ]\n",
      "  [0.366043 0.851272 0.649808 0.212963 0.999818]\n",
      "  [0.732444 0.849242 0.646261 0.215939 1.      ]\n",
      "  [0.257854 0.84934  0.643802 0.213984 1.      ]\n",
      "  [0.659356 0.848977 0.644503 0.216332 0.998855]]\n",
      "\n",
      " [[0.366043 0.848753 0.654525 0.215668 0.99879 ]\n",
      "  [0.732444 0.849586 0.655267 0.214998 0.999584]\n",
      "  [0.257854 0.848828 0.654487 0.215653 1.      ]\n",
      "  [0.659356 0.848485 0.647417 0.213773 1.      ]\n",
      "  [0.484302 0.850918 0.643177 0.217043 0.999788]\n",
      "  [0.366043 0.852348 0.643253 0.220212 1.      ]\n",
      "  [0.732444 0.850103 0.645796 0.216548 0.999484]\n",
      "  [0.257854 0.848334 0.650166 0.214571 0.998233]\n",
      "  [0.659356 0.8484   0.651729 0.215916 1.      ]\n",
      "  [0.484302 0.849496 0.653125 0.214607 0.998674]]\n",
      "\n",
      " [[0.366043 0.84654  0.657153 0.216622 0.999283]\n",
      "  [0.732444 0.850049 0.652476 0.213544 1.      ]\n",
      "  [0.257854 0.848792 0.647634 0.214813 0.998836]\n",
      "  [0.659356 0.851228 0.645484 0.214556 0.996592]\n",
      "  [0.484302 0.848068 0.641635 0.215024 1.      ]\n",
      "  [0.366043 0.848607 0.645667 0.213641 1.      ]\n",
      "  [0.732444 0.84876  0.646454 0.214279 0.999904]\n",
      "  [0.257854 0.847934 0.65039  0.214403 0.997304]\n",
      "  [0.659356 0.849672 0.653907 0.213166 1.      ]\n",
      "  [0.484302 0.849752 0.657577 0.215469 1.      ]]]\n",
      "[[-1.]\n",
      " [ 1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 10\n",
    "window_stride = 1\n",
    "\n",
    "dHandler_valve_window = DamadicsDataHandler(selected_features, window_size, window_stride,\n",
    "                                      start_date_training=start_date_training, end_date_training=end_date_training,\n",
    "                                      start_date_test=start_date_test, end_date_test=end_date_test,\n",
    "                                      binary_classes=True, one_hot_encode=False, samples_per_run=50)\n",
    "dHandler_valve_window.connect_to_db('readOnly', '_readOnly2019', '169.236.181.40', 'damadics')\n",
    "\n",
    "tModel_window = SequenceTunableModelClassification('Damadics_Tree_SK', tree_clf, lib_type='scikit', \n",
    "                                            data_handler=dHandler_valve_window)\n",
    "\n",
    "#tModel.data_scaler = scaler\n",
    "\n",
    "tModel_window.load_data(unroll=False, verbose=1, cross_validation_ratio=0.5, shuffle_samples=True)\n",
    "tModel_window.print_data(print_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['externalControllerOutput', 'pressureValveInlet', 'pressureValveOutlet', 'mediumTemperature', \n",
    "        'rodDisplacement', 'disturbedMediumFlow']\n",
    "\n",
    "cols = ['externalControllerOutput', 'pressureValveInlet', 'pressureValveOutlet', 'mediumTemperature', \n",
    "        'rodDisplacement']\n",
    "\n",
    "cols_fault = cols.copy()\n",
    "cols_fault.append('Fault')\n",
    "\n",
    "X_training = tModel_window.X_train\n",
    "X_training = np.array([np.mean(time_window, axis=0) for time_window in X_training])\n",
    "\n",
    "X_crossVal = tModel_window.X_crossVal\n",
    "X_crossVal = np.array([np.mean(time_window, axis=0) for time_window in X_crossVal])\n",
    "\n",
    "X_test = tModel_window.X_test\n",
    "X_test = np.array([np.mean(time_window, axis=0) for time_window in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108682, 5)\n",
      "[[0.4999998 0.8499483 0.6501955 0.2146905 0.6338918]\n",
      " [0.5366399 0.8496544 0.6483056 0.2157048 0.5698853]\n",
      " [0.4999998 0.8488884 0.6501989 0.2150332 0.6220414]\n",
      " ...\n",
      " [0.4999998 0.8488954 0.6497897 0.214843  0.4942801]\n",
      " [0.4999998 0.8490379 0.6500566 0.215473  0.6337489]\n",
      " [0.4999998 0.8491946 0.6493546 0.2148851 0.6331478]]\n"
     ]
    }
   ],
   "source": [
    "print(X_training.shape)\n",
    "print(X_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tModel_window.X_train = X_training\n",
    "tModel_window.X_crossVal = X_crossVal\n",
    "tModel_window.X_test = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tModel_window.train_model(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3239,)\n",
      "On test set\n",
      "0.6816918802099413\n",
      "On train set\n",
      "0.7827883182127675\n"
     ]
    }
   ],
   "source": [
    "tModel_window.predict_model(cross_validation=True)\n",
    "tModel_window.evaluate_model(cross_validation=True)\n",
    "\n",
    "print(tModel_window.y_predicted.shape)\n",
    "\n",
    "predicted = tModel_window.y_predicted\n",
    "test = tModel_window.y_crossVal\n",
    "test = np.ravel(test)\n",
    "\n",
    "print(\"On test set\")\n",
    "print(accuracy_score(test, predicted))\n",
    "\n",
    "clf = tModel_window.model\n",
    "y_train_pred = clf.predict(tModel_window.X_train)\n",
    "\n",
    "train = tModel_window.y_train\n",
    "train = np.ravel(train)\n",
    "\n",
    "print(\"On train set\")\n",
    "print(accuracy_score(tModel_window.y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN with window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "window_stride = 1\n",
    "input_shape = (window_size * len(selected_features), )\n",
    "# print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_m():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=128, activation='sigmoid', input_shape = (5, )))\n",
    "    model.add(Dense(units=32, activation='sigmoid'))\n",
    "    model.add(Dense(units=8, activation='sigmoid'))\n",
    "    model.add(Dense(units=2))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to mysql+mysqldb://readOnly:_readOnly2019@169.236.181.40/damadics successfull\n",
      "Loading data for the first time\n",
      "Reloading data due to parameter change\n",
      "Loading training data for DAMADICS with window_size of 10, stride of 1. Cros-Validation ratio 0.5\n",
      "Loading data from database\n",
      "Reading data from ValveReading\n",
      "2019-06-14 17:06:41\n",
      "2019-10-31 14:26:41\n",
      "Extracting data from database runtime: 0:00:01.364161\n",
      "Data Splitting: 0:00:00.000100\n",
      "Loading test data for DAMADICS with window_size of 10, stride of 1\n",
      "Loading data from database\n",
      "Reading data from ValveReadingTest\n",
      "Extracting data from database runtime: 0:00:01.248162\n",
      "Printing shapes\n",
      "\n",
      "Training data (X, y)\n",
      "(96415, 10, 5)\n",
      "(96415, 2)\n",
      "Cross-Validation data (X, y)\n",
      "(3276, 10, 5)\n",
      "(3276, 2)\n",
      "Testing data (X, y)\n",
      "(7400, 10, 5)\n",
      "(7400, 2)\n",
      "Printing first 5 elements\n",
      "\n",
      "Training data (X, y)\n",
      "[[[0.366043 0.849679 0.650034 0.215014 0.668619]\n",
      "  [0.732444 0.848966 0.652152 0.215697 0.743013]\n",
      "  [0.257854 0.850063 0.655861 0.216233 0.50106 ]\n",
      "  [0.659356 0.850119 0.656262 0.215012 0.832527]\n",
      "  [0.484302 0.850497 0.654115 0.216852 0.472315]\n",
      "  [0.366043 0.850922 0.651039 0.215521 0.657784]\n",
      "  [0.732444 0.848611 0.648852 0.215317 0.719128]\n",
      "  [0.257854 0.848658 0.644878 0.212885 0.475275]\n",
      "  [0.484302 0.850865 0.643815 0.213335 0.455493]\n",
      "  [0.366043 0.848311 0.647095 0.214248 0.663863]]\n",
      "\n",
      " [[0.257854 0.847807 0.649304 0.214563 1.      ]\n",
      "  [0.659356 0.848348 0.65376  0.217493 0.859622]\n",
      "  [0.484302 0.848287 0.656777 0.215007 0.999531]\n",
      "  [0.366043 0.849866 0.656722 0.213837 1.      ]\n",
      "  [0.732444 0.849474 0.652447 0.21411  0.      ]\n",
      "  [0.257854 0.849126 0.650685 0.216509 0.999521]\n",
      "  [0.659356 0.849179 0.645028 0.214999 0.212966]\n",
      "  [0.484302 0.848564 0.64603  0.216313 0.999847]\n",
      "  [0.366043 0.848609 0.643492 0.215029 0.832354]\n",
      "  [0.732444 0.851438 0.642183 0.213623 1.      ]]\n",
      "\n",
      " [[0.732444 0.851074 0.644262 0.214432 0.725541]\n",
      "  [0.257854 0.85038  0.648774 0.214884 0.477939]\n",
      "  [0.659356 0.84901  0.650777 0.217881 0.811712]\n",
      "  [0.484302 0.849856 0.652419 0.212224 0.465381]\n",
      "  [0.366043 0.84702  0.656615 0.213175 0.674404]\n",
      "  [0.732444 0.850634 0.656656 0.213024 0.738915]\n",
      "  [0.257854 0.85097  0.651769 0.214066 0.505842]\n",
      "  [0.659356 0.851476 0.652217 0.215445 0.827045]\n",
      "  [0.484302 0.850826 0.647839 0.214973 0.460591]\n",
      "  [0.366043 0.847898 0.644413 0.215175 0.645597]]\n",
      "\n",
      " [[0.659356 0.847851 0.65562  0.216312 0.942276]\n",
      "  [0.484302 0.848921 0.651176 0.218577 0.592264]\n",
      "  [0.366043 0.850313 0.642444 0.216093 0.665361]\n",
      "  [0.732444 0.849155 0.642729 0.216944 0.835453]\n",
      "  [0.257854 0.850352 0.64176  0.214411 0.44156 ]\n",
      "  [0.484302 0.849514 0.649571 0.216261 0.552958]\n",
      "  [0.366043 0.847557 0.653183 0.215951 0.7069  ]\n",
      "  [0.732444 0.849632 0.656385 0.213918 0.880775]\n",
      "  [0.257854 0.849828 0.656985 0.21725  0.462644]\n",
      "  [0.659356 0.848291 0.652218 0.215593 0.943426]]\n",
      "\n",
      " [[0.257854 0.84968  0.651391 0.214886 0.491477]\n",
      "  [0.659356 0.848107 0.647384 0.216779 0.818832]\n",
      "  [0.484302 0.846244 0.641393 0.21556  0.455155]\n",
      "  [0.366043 0.850231 0.644233 0.214717 0.654668]\n",
      "  [0.732444 0.851394 0.646405 0.216079 0.732177]\n",
      "  [0.257854 0.849673 0.650295 0.214816 0.485115]\n",
      "  [0.659356 0.849121 0.655042 0.213131 0.821125]\n",
      "  [0.484302 0.850884 0.655506 0.216014 0.472043]\n",
      "  [0.366043 0.849179 0.655635 0.217644 0.666425]\n",
      "  [0.732444 0.849894 0.652245 0.217191 0.729883]]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "Cross-Validation data (X, y)\n",
      "[[[0.366043 0.848858 0.653306 0.214766 0.672927]\n",
      "  [0.732444 0.844543 0.65445  0.214119 0.744469]\n",
      "  [0.257854 0.851445 0.656624 0.214072 0.514133]\n",
      "  [0.659356 0.850077 0.656034 0.21673  0.832738]\n",
      "  [0.484302 0.84943  0.652511 0.216151 0.470402]\n",
      "  [0.366043 0.85087  0.649932 0.216738 0.652645]\n",
      "  [0.732444 0.849895 0.645245 0.215387 0.717451]\n",
      "  [0.257854 0.84858  0.643274 0.218033 0.46773 ]\n",
      "  [0.659356 0.850714 0.642924 0.214471 0.807899]\n",
      "  [0.484302 0.848548 0.646533 0.214597 0.459636]]\n",
      "\n",
      " [[0.484302 0.850501 0.652234 0.213916 0.467174]\n",
      "  [0.366043 0.851    0.655111 0.21519  0.675274]\n",
      "  [0.732444 0.849694 0.655581 0.213518 0.741808]\n",
      "  [0.257854 0.848424 0.655633 0.215261 0.505878]\n",
      "  [0.659356 0.850087 0.649796 0.21585  0.827722]\n",
      "  [0.484302 0.849303 0.646938 0.218199 0.462855]\n",
      "  [0.366043 0.850217 0.641748 0.214954 0.649082]\n",
      "  [0.732444 0.849941 0.642579 0.214855 0.720696]\n",
      "  [0.257854 0.849556 0.645344 0.218176 0.471231]\n",
      "  [0.659356 0.845464 0.647892 0.213452 0.810934]]\n",
      "\n",
      " [[0.257854 0.850998 0.649085 0.216482 0.240343]\n",
      "  [0.659356 0.852978 0.64721  0.215537 0.706989]\n",
      "  [0.484302 0.847374 0.643839 0.215377 0.46684 ]\n",
      "  [0.366043 0.847058 0.645541 0.216228 0.410104]\n",
      "  [0.732444 0.848142 0.647407 0.215659 0.730898]\n",
      "  [0.257854 0.848233 0.649833 0.215171 0.240705]\n",
      "  [0.659356 0.850026 0.655419 0.216557 0.708003]\n",
      "  [0.484302 0.849515 0.65734  0.216733 0.468718]\n",
      "  [0.366043 0.852914 0.654265 0.215431 0.424697]\n",
      "  [0.732444 0.853049 0.65318  0.217493 0.730946]]\n",
      "\n",
      " [[0.257854 0.847143 0.642855 0.215037 0.471187]\n",
      "  [0.659356 0.848128 0.644314 0.213696 0.811681]\n",
      "  [0.484302 0.847511 0.643634 0.213644 0.458091]\n",
      "  [0.366043 0.850171 0.649654 0.214372 0.664949]\n",
      "  [0.732444 0.848261 0.650028 0.214163 0.740596]\n",
      "  [0.257854 0.849236 0.655838 0.216718 0.504173]\n",
      "  [0.659356 0.849133 0.657682 0.21486  0.830131]\n",
      "  [0.484302 0.847896 0.657268 0.214951 0.477532]\n",
      "  [0.366043 0.849336 0.651161 0.213666 0.657041]\n",
      "  [0.732444 0.850112 0.64803  0.214525 0.722211]]\n",
      "\n",
      " [[0.732444 0.847277 0.644677 0.217436 0.714535]\n",
      "  [0.257854 0.850025 0.643491 0.214792 0.468902]\n",
      "  [0.659356 0.848588 0.644914 0.212801 0.809168]\n",
      "  [0.484302 0.850088 0.647087 0.214679 0.461037]\n",
      "  [0.366043 0.849146 0.650404 0.215523 0.671825]\n",
      "  [0.732444 0.849099 0.652946 0.213237 0.744907]\n",
      "  [0.257854 0.850862 0.655322 0.215926 0.511355]\n",
      "  [0.659356 0.847909 0.65595  0.215041 0.834269]\n",
      "  [0.484302 0.850423 0.653706 0.216975 0.471112]\n",
      "  [0.366043 0.84892  0.648087 0.217179 0.652596]]]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "Testing data (X, y)\n",
      "[[[0.257854 0.850498 0.645263 0.215341 0.999359]\n",
      "  [0.659356 0.849617 0.644176 0.215967 0.999293]\n",
      "  [0.484302 0.846793 0.645857 0.214509 0.999567]\n",
      "  [0.366043 0.848298 0.647953 0.215485 1.      ]\n",
      "  [0.732444 0.848024 0.651494 0.215434 1.      ]\n",
      "  [0.257854 0.851621 0.654469 0.217643 1.      ]\n",
      "  [0.659356 0.849134 0.65773  0.217321 1.      ]\n",
      "  [0.484302 0.849927 0.655716 0.217066 0.998423]\n",
      "  [0.366043 0.849279 0.654116 0.218665 1.      ]\n",
      "  [0.732444 0.849453 0.649337 0.214914 0.999608]]\n",
      "\n",
      " [[0.659356 0.849762 0.647518 0.215748 1.      ]\n",
      "  [0.484302 0.847471 0.647871 0.214055 0.999901]\n",
      "  [0.366043 0.851176 0.651585 0.216824 1.      ]\n",
      "  [0.732444 0.850811 0.654861 0.213269 0.998691]\n",
      "  [0.257854 0.84961  0.655047 0.2161   1.      ]\n",
      "  [0.659356 0.850312 0.653673 0.216291 0.998952]\n",
      "  [0.484302 0.847967 0.649951 0.217594 1.      ]\n",
      "  [0.366043 0.852474 0.647187 0.21615  0.998995]\n",
      "  [0.732444 0.845544 0.641603 0.216796 0.998234]\n",
      "  [0.257854 0.84901  0.640819 0.213463 1.      ]]\n",
      "\n",
      " [[0.257854 0.850763 0.660883 0.212981 0.29339 ]\n",
      "  [0.659356 0.84796  0.656156 0.214383 0.80549 ]\n",
      "  [0.484302 0.850586 0.654338 0.214226 0.462073]\n",
      "  [0.366043 0.850349 0.652732 0.215945 0.534268]\n",
      "  [0.732444 0.847672 0.645861 0.214473 0.732845]\n",
      "  [0.257854 0.849574 0.644606 0.213389 0.288272]\n",
      "  [0.659356 0.852161 0.645066 0.216341 0.794909]\n",
      "  [0.484302 0.851489 0.647809 0.21525  0.467467]\n",
      "  [0.366043 0.850719 0.65052  0.215388 0.55026 ]\n",
      "  [0.732444 0.849185 0.653844 0.213422 0.748096]]\n",
      "\n",
      " [[0.732444 0.851074 0.658078 0.217576 0.998747]\n",
      "  [0.257854 0.849408 0.65439  0.215653 0.999975]\n",
      "  [0.659356 0.847617 0.655062 0.215263 0.999594]\n",
      "  [0.484302 0.85245  0.650521 0.215771 1.      ]\n",
      "  [0.366043 0.847724 0.645488 0.21505  1.      ]\n",
      "  [0.732444 0.847145 0.642393 0.215676 1.      ]\n",
      "  [0.257854 0.850278 0.643835 0.214228 0.999843]\n",
      "  [0.659356 0.851296 0.643435 0.214223 0.999195]\n",
      "  [0.484302 0.851584 0.652526 0.213469 1.      ]\n",
      "  [0.366043 0.849439 0.653214 0.212203 1.      ]]\n",
      "\n",
      " [[0.732444 0.848893 0.6421   0.212965 1.      ]\n",
      "  [0.257854 0.849699 0.645566 0.214188 1.      ]\n",
      "  [0.659356 0.851135 0.648548 0.214664 1.      ]\n",
      "  [0.484302 0.850149 0.653518 0.21495  0.998424]\n",
      "  [0.366043 0.850639 0.65741  0.214871 1.      ]\n",
      "  [0.732444 0.848801 0.658003 0.21498  0.999761]\n",
      "  [0.257854 0.851187 0.654575 0.217612 1.      ]\n",
      "  [0.659356 0.848544 0.652835 0.214745 1.      ]\n",
      "  [0.484302 0.848376 0.645224 0.214267 0.998519]\n",
      "  [0.366043 0.851075 0.6456   0.2136   1.      ]]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "dHandler_valve_nn = DamadicsDataHandler(selected_features, window_size, window_stride,\n",
    "                                      start_date_training=start_date_training, end_date_training=end_date_training,\n",
    "                                      start_date_test=start_date_test, end_date_test=end_date_test,\n",
    "                                      binary_classes=True, one_hot_encode=True, samples_per_run=50)\n",
    "dHandler_valve_nn.connect_to_db('readOnly', '_readOnly2019', '169.236.181.40', 'damadics')\n",
    "\n",
    "model = model_m()\n",
    "optimizer = Adam(lr=0.001, beta_1=0.5)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "tModel_nn = SequenceTunableModelClassification('Damadics_nn', model, lib_type='keras', \n",
    "                                            data_handler=dHandler_valve_nn)\n",
    "\n",
    "#tModel.data_scaler = scaler\n",
    "\n",
    "tModel_nn.load_data(unroll=False, verbose=1, cross_validation_ratio=0.5, shuffle_samples=True)\n",
    "tModel_nn.print_data(print_top=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['externalControllerOutput', 'pressureValveInlet', 'pressureValveOutlet', 'mediumTemperature', \n",
    "        'rodDisplacement', 'disturbedMediumFlow']\n",
    "\n",
    "cols = ['externalControllerOutput', 'pressureValveInlet', 'pressureValveOutlet', 'mediumTemperature', \n",
    "        'rodDisplacement']\n",
    "\n",
    "cols_fault = cols.copy()\n",
    "cols_fault.append('Fault')\n",
    "\n",
    "X_training = tModel_nn.X_train\n",
    "X_training = np.array([np.mean(time_window, axis=0) for time_window in X_training])\n",
    "\n",
    "X_crossVal = tModel_nn.X_crossVal\n",
    "X_crossVal = np.array([np.mean(time_window, axis=0) for time_window in X_crossVal])\n",
    "\n",
    "X_test = tModel_nn.X_test\n",
    "X_test = np.array([np.mean(time_window, axis=0) for time_window in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96415, 5)\n"
     ]
    }
   ],
   "source": [
    "tModel_nn.X_train = X_training\n",
    "tModel_nn.X_crossVal = X_crossVal\n",
    "tModel_nn.X_test = X_test\n",
    "\n",
    "print(X_training.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with cv\n",
      "Train on 96415 samples, validate on 3276 samples\n",
      "Epoch 1/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 2/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 3/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 4/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 5/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 6/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 7/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 8/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 9/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 10/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 11/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 12/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 13/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 14/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 15/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 16/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 17/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 18/250\n",
      "96415/96415 [==============================] - 0s 3us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 19/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 20/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 21/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 22/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 23/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 24/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 25/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 26/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 27/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 28/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 29/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 30/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 31/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 32/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 33/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 34/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 35/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 36/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 37/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 38/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 39/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 40/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 41/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 42/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 43/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 44/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 45/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 46/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 47/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 48/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 49/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 50/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 51/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 52/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 53/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 54/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 55/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 56/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 57/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 58/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 59/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 61/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 62/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 63/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 64/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 65/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 66/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 67/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 68/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 69/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 70/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 71/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 72/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 73/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 74/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 75/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 76/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 77/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 78/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 79/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 80/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 81/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 82/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 83/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 84/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 85/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 86/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 87/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 88/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 89/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 90/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 91/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 92/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 93/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 94/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 95/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 96/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 97/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 98/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 99/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 100/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 101/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 102/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 103/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 104/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 105/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 106/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 107/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 108/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 109/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 110/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 111/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 112/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 113/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 114/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 115/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 116/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 117/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 118/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 119/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 120/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 121/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 122/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 123/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 124/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 125/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 126/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 127/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 128/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 129/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 130/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 131/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 132/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 133/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 134/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 135/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 136/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 137/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 138/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 139/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 140/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 141/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 142/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 143/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 144/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 145/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 146/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 147/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 148/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 149/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 150/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 151/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 152/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 153/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 154/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 155/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 156/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 157/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 158/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 159/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 160/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 161/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 162/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 163/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 164/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 165/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 166/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 167/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 168/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 169/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 170/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 171/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 172/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 173/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 174/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 175/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 176/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 177/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 178/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 179/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 180/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 181/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 182/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 183/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 184/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 185/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 186/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 187/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 188/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 189/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 190/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 191/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 192/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 193/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 194/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 195/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 196/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 197/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 198/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 199/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 200/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 201/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 202/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 203/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 204/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 205/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 206/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 207/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 208/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 209/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 210/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 211/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 212/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 213/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 214/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 215/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 216/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 217/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 218/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 219/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 220/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 221/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 222/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 223/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 224/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 225/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 226/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 227/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 228/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 229/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 230/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 231/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 232/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 233/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 234/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 235/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 236/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 237/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 238/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 239/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 240/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 241/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 242/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 243/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 244/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 245/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 246/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 247/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 248/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 249/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n",
      "Epoch 250/250\n",
      "96415/96415 [==============================] - 0s 2us/step - loss: 3.4896 - acc: 0.7549 - val_loss: 6.1009 - val_acc: 0.6215\n"
     ]
    }
   ],
   "source": [
    "tModel_nn.train_model(verbose=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7400/7400 [==============================] - 0s 6us/step\n",
      "(7400, 2)\n",
      "{'loss': 5.996367162756018, 'score_1': 0.6201351351351352}\n"
     ]
    }
   ],
   "source": [
    "tModel_nn.predict_model(cross_validation=False)\n",
    "tModel_nn.evaluate_model(cross_validation=False)\n",
    "\n",
    "print(tModel_nn.y_predicted.shape)\n",
    "print(tModel_nn.scores)\n",
    "# predicted = tModel_nn.y_predicted\n",
    "# test = tModel_nn.y_crossVal\n",
    "# test = np.ravel(test)\n",
    "\n",
    "# print(\"On test set\")\n",
    "# print(accuracy_score(test, predicted))\n",
    "\n",
    "# clf = tModel_nn.model\n",
    "# y_train_pred = clf.predict(tModel_nn.X_train)\n",
    "\n",
    "# train = tModel_nn.y_train\n",
    "# train = np.ravel(train)\n",
    "\n",
    "# print(\"On train set\")\n",
    "# print(accuracy_score(tModel_nn.y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
